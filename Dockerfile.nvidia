# ============================================================================
# Cogito - Self-Correcting RAG System
# NVIDIA GPU Dockerfile — uses pre-built CUDA wheels from JamePeng fork
# ============================================================================
# Uses pre-built llama-cpp-python CUDA wheel from:
#   https://github.com/JamePeng/llama-cpp-python/releases
#
# Advantages:
#   - No CUDA devel image needed (no nvcc / cmake / build-essential)
#   - Build completes in minutes instead of 15-20 min compilation
#   - Still uses the required JamePeng/llama-cpp-python fork
#   - The wheel bundles all GPU architectures (SM75–SM90+)
#
# Prerequisites:
#   - NVIDIA GPU drivers ≥ 525.60.13 (for CUDA 12.4)
#   - NVIDIA Container Toolkit installed
#     https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html
#
# Build & Run:
#   docker compose -f docker-compose.nvidia.yml build
#   docker compose -f docker-compose.nvidia.yml up
#
# To update llama-cpp-python version, change LLAMA_CPP_VERSION build arg:
#   docker compose -f docker-compose.nvidia.yml build \
#     --build-arg LLAMA_CPP_VERSION=0.3.25 \
#     --build-arg LLAMA_CPP_RELEASE_DATE=20260301
#
# Verify GPU access:
#   docker exec cogito-nvidia nvidia-smi
# ============================================================================


# ---------------------------------------------------------------------------
# Stage 1: Builder — install deps + pre-built CUDA wheel (no compilation)
# ---------------------------------------------------------------------------
# python:3.11-slim is sufficient — we're only downloading wheels, not compiling.
# No need for the 4GB nvidia/cuda:devel image.
FROM python:3.11-slim AS builder

ENV DEBIAN_FRONTEND=noninteractive

# Build args for the llama-cpp-python pre-built wheel
# Update these to pick up new releases from:
#   https://github.com/JamePeng/llama-cpp-python/releases
ARG LLAMA_CPP_VERSION=0.3.24
ARG CUDA_SHORT=cu124
ARG LLAMA_CPP_RELEASE_DATE=20260208
ARG LLAMA_CPP_WHEEL_URL="https://github.com/JamePeng/llama-cpp-python/releases/download/v${LLAMA_CPP_VERSION}-${CUDA_SHORT}-Basic-linux-${LLAMA_CPP_RELEASE_DATE}/llama_cpp_python-${LLAMA_CPP_VERSION}+${CUDA_SHORT}.basic-cp311-cp311-linux_x86_64.whl"

WORKDIR /build
COPY requirements.txt .

# Create venv, install all deps except llama-cpp-python,
# then install the pre-built CUDA wheel from the fork's releases
RUN pip install --no-cache-dir --upgrade pip setuptools wheel \
    && python -m venv /opt/venv \
    && /opt/venv/bin/pip install --no-cache-dir --upgrade pip setuptools wheel \
    && grep -vi "llama-cpp-python" requirements.txt > requirements-filtered.txt \
    && /opt/venv/bin/pip install --no-cache-dir -r requirements-filtered.txt \
    && /opt/venv/bin/pip install --no-cache-dir "${LLAMA_CPP_WHEEL_URL}"

# Clean up to reduce image size
RUN find /opt/venv -type d -name '__pycache__' -exec rm -rf {} + 2>/dev/null || true \
    && find /opt/venv -type d -name 'tests' -exec rm -rf {} + 2>/dev/null || true \
    && find /opt/venv -type f -name '*.pyc' -delete \
    && find /opt/venv -type f -name '*.pyo' -delete \
    && rm -rf /opt/venv/share/man/* \
    && rm -rf /opt/venv/lib/python3.11/site-packages/pip


# ---------------------------------------------------------------------------
# Stage 2: Runtime — CUDA runtime image for GPU inference
# ---------------------------------------------------------------------------
# nvidia/cuda:12.4.1-runtime includes libcudart.
# We add libcublas-12-4 for GPU-accelerated matrix ops required by llama.cpp.
FROM nvidia/cuda:12.4.1-runtime-ubuntu22.04 AS runtime

ENV DEBIAN_FRONTEND=noninteractive \
    PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    PATH="/opt/venv/bin:$PATH" \
    LD_LIBRARY_PATH="/usr/local/cuda/lib64:${LD_LIBRARY_PATH}"

# Minimal runtime deps
# libgomp1:       OpenMP support for llama.cpp parallel inference
# libcublas-12-4: cuBLAS runtime for GPU-accelerated matrix ops
# curl:           healthcheck
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3.11 \
    python3.11-venv \
    libgomp1 \
    libcublas-12-4 \
    curl \
    ca-certificates \
    && rm -rf /var/lib/apt/lists/* \
    && apt-get clean \
    && rm -rf /var/cache/apt/archives/* /tmp/*

RUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 1 \
    && update-alternatives --install /usr/bin/python python /usr/bin/python3.11 1

# Copy pre-built virtual environment from builder
COPY --from=builder /opt/venv /opt/venv

# Create non-root user and application directories
RUN groupadd --gid 1000 cogito \
    && useradd --uid 1000 --gid 1000 --shell /bin/bash --create-home cogito \
    && mkdir -p \
        /app/data/raw/uploads \
        /app/data/processed \
        /app/db/chroma \
        /app/db/graph \
        /app/models \
        /app/logs \
        /app/results \
    && chown -R cogito:cogito /app

WORKDIR /app

# Copy application code
COPY --chown=cogito:cogito config/ ./config/
COPY --chown=cogito:cogito src/ ./src/
COPY --chown=cogito:cogito utils/ ./utils/
COPY --chown=cogito:cogito scripts/ ./scripts/

# Switch to non-root user
USER cogito

EXPOSE 8000

# Override the NVIDIA base image entrypoint (nvidia_entrypoint.sh)
ENTRYPOINT []

# Healthcheck — increased start-period for model loading
HEALTHCHECK --interval=30s --timeout=10s --start-period=90s --retries=3 \
    CMD curl -f http://localhost:8000/health || curl -f http://localhost:8000/ || exit 1

LABEL maintainer="cogito-dev" \
      version="1.0" \
      description="Cogito Self-Correcting RAG System with CUDA 12.4.1" \
      cuda.version="12.4.1" \
      python.version="3.11"

# Run FastAPI server (single worker — model is not fork-safe)
CMD ["uvicorn", "src.frontend.app:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "1"]
